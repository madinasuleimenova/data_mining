{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdd3c805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "426f7054",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdDf = pd.read_csv('C:\\\\Users\\\\sulej\\\\OneDrive\\\\Рабочий стол\\\\7 semester\\\\Data Mining\\\\Titanic-Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be2b190d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived  Pclass  \\\n",
      "0              1         0       3   \n",
      "1              2         1       1   \n",
      "2              3         1       3   \n",
      "3              4         1       1   \n",
      "4              5         0       3   \n",
      "..           ...       ...     ...   \n",
      "886          887         0       2   \n",
      "887          888         1       1   \n",
      "888          889         0       3   \n",
      "889          890         1       1   \n",
      "890          891         0       3   \n",
      "\n",
      "                                                  Name     Sex   Age  SibSp  \\\n",
      "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                             Allen, Mr. William Henry    male  35.0      0   \n",
      "..                                                 ...     ...   ...    ...   \n",
      "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
      "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
      "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
      "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
      "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
      "\n",
      "     Parch            Ticket     Fare Cabin Embarked  \n",
      "0        0         A/5 21171   7.2500   NaN        S  \n",
      "1        0          PC 17599  71.2833   C85        C  \n",
      "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3        0            113803  53.1000  C123        S  \n",
      "4        0            373450   8.0500   NaN        S  \n",
      "..     ...               ...      ...   ...      ...  \n",
      "886      0            211536  13.0000   NaN        S  \n",
      "887      0            112053  30.0000   B42        S  \n",
      "888      2        W./C. 6607  23.4500   NaN        S  \n",
      "889      0            111369  30.0000  C148        C  \n",
      "890      0            370376   7.7500   NaN        Q  \n",
      "\n",
      "[891 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "print(pdDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc41bc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId\n",
      "Survived\n",
      "Pclass\n",
      "Name\n",
      "Sex\n",
      "Age\n",
      "SibSp\n",
      "Parch\n",
      "Ticket\n",
      "Fare\n",
      "Cabin\n",
      "Embarked\n"
     ]
    }
   ],
   "source": [
    "for col in pdDf.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4efcc67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target variable\n",
    "X = pdDf.drop(columns=['Survived'])\n",
    "y = pdDf['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2114f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 623\n",
      "Testing set size: 268\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets with a 70-30 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Print the sizes of the resulting sets\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bd48812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size (80-20): 712\n",
      "Testing set size (80-20): 179\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets with an 80-20 ratio\n",
    "X_train_80, X_test_80, y_train_80, y_test_80 = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the sizes of the resulting sets\n",
    "print(f\"Training set size (80-20): {X_train_80.shape[0]}\")\n",
    "print(f\"Testing set size (80-20): {X_test_80.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eff59b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 70-30 split: 0.80\n",
      "Accuracy with 80-20 split: 0.79\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the dataset\n",
    "pdDf = pd.read_csv('C:\\\\Users\\\\sulej\\\\OneDrive\\\\Рабочий стол\\\\7 semester\\\\Data Mining\\\\Titanic-Dataset.csv')\n",
    "\n",
    "# Define features and target variable\n",
    "X = pdDf.drop(columns=['Survived'])\n",
    "y = pdDf['Survived']\n",
    "\n",
    "# Define preprocessing for numerical and categorical features\n",
    "numerical_features = ['Age', 'SibSp', 'Parch', 'Fare']\n",
    "categorical_features = ['Sex', 'Embarked']\n",
    "\n",
    "# Create transformers for numerical and categorical data\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine transformers into a single preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing to features\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Split the preprocessed data with 70-30 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define and train the model\n",
    "model_70_30 = LogisticRegression(max_iter=1000)\n",
    "model_70_30.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_70_30 = model_70_30.predict(X_test)\n",
    "accuracy_70_30 = accuracy_score(y_test, y_pred_70_30)\n",
    "\n",
    "print(f\"Accuracy with 70-30 split: {accuracy_70_30:.2f}\")\n",
    "\n",
    "# Split the preprocessed data with 80-20 ratio\n",
    "X_train_80, X_test_80, y_train_80, y_test_80 = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define and train the model\n",
    "model_80_20 = LogisticRegression(max_iter=1000)\n",
    "model_80_20.fit(X_train_80, y_train_80)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_80_20 = model_80_20.predict(X_test_80)\n",
    "accuracy_80_20 = accuracy_score(y_test_80, y_pred_80_20)\n",
    "\n",
    "print(f\"Accuracy with 80-20 split: {accuracy_80_20:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26f58394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define preprocessing for numerical and categorical features\n",
    "numerical_features = ['Age', 'SibSp', 'Parch', 'Fare']\n",
    "categorical_features = ['Sex', 'Embarked']\n",
    "\n",
    "# Create transformers for numerical and categorical data\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine transformers into a single preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing to features\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Split the preprocessed data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21e257d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define and train the model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c71d645b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.80\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84       157\n",
      "           1       0.80      0.70      0.75       111\n",
      "\n",
      "    accuracy                           0.80       268\n",
      "   macro avg       0.80      0.79      0.79       268\n",
      "weighted avg       0.80      0.80      0.80       268\n",
      "\n",
      "Confusion Matrix:\n",
      "[[137  20]\n",
      " [ 33  78]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Additional metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5060e6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 80-20 split: 0.79\n",
      "Classification Report for 80-20 split:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.86      0.83       105\n",
      "           1       0.77      0.69      0.73        74\n",
      "\n",
      "    accuracy                           0.79       179\n",
      "   macro avg       0.78      0.77      0.78       179\n",
      "weighted avg       0.79      0.79      0.79       179\n",
      "\n",
      "Confusion Matrix for 80-20 split:\n",
      "[[90 15]\n",
      " [23 51]]\n"
     ]
    }
   ],
   "source": [
    "# Split the preprocessed data with an 80-20 ratio\n",
    "X_train_80, X_test_80, y_train_80, y_test_80 = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model with the 80-20 split\n",
    "model_80_20 = LogisticRegression(max_iter=1000)\n",
    "model_80_20.fit(X_train_80, y_train_80)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_80_20 = model_80_20.predict(X_test_80)\n",
    "accuracy_80_20 = accuracy_score(y_test_80, y_pred_80_20)\n",
    "\n",
    "print(f\"Accuracy with 80-20 split: {accuracy_80_20:.2f}\")\n",
    "\n",
    "# Additional metrics for the 80-20 split\n",
    "print(\"Classification Report for 80-20 split:\")\n",
    "print(classification_report(y_test_80, y_pred_80_20))\n",
    "\n",
    "print(\"Confusion Matrix for 80-20 split:\")\n",
    "print(confusion_matrix(y_test_80, y_pred_80_20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b882201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Define preprocessing for numerical and categorical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine transformers into a single preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing to features\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1ac96bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 1, 'solver': 'liblinear'}\n",
      "Best cross-validation score: 0.7889962965287804\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a parameter grid for tuning\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['liblinear', 'newton-cg', 'lbfgs']  # Exclude 'saga' if it's problematic\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with an increased max_iter\n",
    "grid_search = GridSearchCV(LogisticRegression(max_iter=2000), param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit grid search\n",
    "grid_search.fit(X_preprocessed, y)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "290feda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the best model: 0.70\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.59      0.69       157\n",
      "           1       0.59      0.86      0.70       111\n",
      "\n",
      "    accuracy                           0.70       268\n",
      "   macro avg       0.72      0.72      0.70       268\n",
      "weighted avg       0.74      0.70      0.70       268\n",
      "\n",
      "Confusion Matrix:\n",
      "[[92 65]\n",
      " [16 95]]\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the best model from GridSearchCV\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of the best model: {accuracy:.2f}\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43d3972",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
